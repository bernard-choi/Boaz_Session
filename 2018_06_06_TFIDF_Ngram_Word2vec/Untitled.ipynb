{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram\n",
    "# n-gram 은 n개 어절/음절을 연쇄적으로 분류해 그 빈도를 따진다\n",
    "# n=1 일 때는 unigram, 2일 때는 bigram, 3 - trigram \n",
    "# 긴 텍스트를 분석하게 된다면 같은 n-gram 이 여러개 나온다 \n",
    "# n-gram 들의 수를 세서 리스트로 만든 것을 빈도 리스트 (frequency list) 라고 한다 \n",
    "# 이상의 날개에서 가장 많이 나온 n-gram 이 뭘까 궁금해서 Python으로 n-gram 분석을 해봤습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nalgae.txt', 'r', encoding='utf-8', newline='\\n') as f:\n",
    "    file = f.read()\n",
    "sample_file = ''.join(file)[6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"박제(剝製)가 되어 버린 천재'를 아시오? 나는 유쾌하오. 이런 때 연애까지가 유쾌하오.\\r\\n육신이 흐느적흐느적하도록 피로했을 때만 정신이 은화처럼 맑소. 니코틴이 내 횟배 앓는 뱃속\\r\\n으로 스미면 머릿속에 으레 백지가 준비되는 법이오. 그 위에다 나는 위트와 파라독스를 바둑 포\\r\\n석처럼 늘어 놓소. 가공할 상식의 병이오.\\r\\n나는 또 여인과 생활을 설계하오. 연애기법에마저 서먹서먹해진 지성의 극치를 흘깃 좀 들여다\\r\\n본 일이 있는, 말하자면 일종의 정신분일자(정신이 제멋대로 노는 사람)말이오. 이런 여인의 반\\r\\n----그것은 온갖 것\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어절 n-gram 분석\n",
    "#sentence: 분석할 문장, num_gram: n-gram 단위\n",
    "def word_ngram(sentence, num_gram):\n",
    "    # in the case a file is given, remove escape characters\n",
    "    sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('  ',' ')\n",
    "    text = tuple(sentence.split(' '))\n",
    "    ngrams = [text[x:x+num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('박제(剝製)가', '되어'),\n",
       " ('되어', '버린'),\n",
       " ('버린', \"천재'를\"),\n",
       " (\"천재'를\", '아시오?'),\n",
       " ('아시오?', '나는'),\n",
       " ('나는', '유쾌하오.'),\n",
       " ('유쾌하오.', '이런'),\n",
       " ('이런', '때'),\n",
       " ('때', '연애까지가'),\n",
       " ('연애까지가', '유쾌하오.'),\n",
       " ('유쾌하오.', '육신이'),\n",
       " ('육신이', '흐느적흐느적하도록'),\n",
       " ('흐느적흐느적하도록', '피로했을'),\n",
       " ('피로했을', '때만'),\n",
       " ('때만', '정신이'),\n",
       " ('정신이', '은화처럼'),\n",
       " ('은화처럼', '맑소.'),\n",
       " ('맑소.', '니코틴이'),\n",
       " ('니코틴이', '내'),\n",
       " ('내', '횟배'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ngram(sample_file,2)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음절 n-gram 분석\n",
    "#sentence: 분석할 문장, num_gram: n-gram 단위\n",
    "def phoneme_ngram(sentence, num_gram):\n",
    "    text = tuple(sentence) # split the sentence into an array of characters\n",
    "    ngrams = [text[x:x+num_gram] for x in range(0, len(text))]\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('박', '제', '('),\n",
       " ('제', '(', '剝'),\n",
       " ('(', '剝', '製'),\n",
       " ('剝', '製', ')'),\n",
       " ('製', ')', '가'),\n",
       " (')', '가', ' '),\n",
       " ('가', ' ', '되'),\n",
       " (' ', '되', '어'),\n",
       " ('되', '어', ' '),\n",
       " ('어', ' ', '버')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_ngram(sample_file,3)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram 빈도 리스트 생성\n",
    "def make_freqlist(ngrams):\n",
    "    freqlist = {}\n",
    " \n",
    "    for ngram in ngrams:\n",
    "        if (ngram in freqlist):\n",
    "            freqlist[ngram] += 1\n",
    "        else:\n",
    "            freqlist[ngram] = 1\n",
    "    return freqlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = word_ngram(sample_file, 3)\n",
    "freqlist = make_freqlist(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('알', '수', '없다.'), 4), (('그', '돈', '오'), 4), (('돈을', '놓고', '가는'), 3), (('것', '같았다.', '나는'), 3), (('오', '원', '돈을'), 3), (('해가', '드는', '것을'), 2), (('수', '없다.', '나는'), 2), (('나는', '내', '아내'), 2), (('내', '아내', '외의'), 2), (('아내', '외의', '다른'), 2)]\n"
     ]
    }
   ],
   "source": [
    "#sorted_freqlist = sorted(freqlist.items(), key=operator.itemgetter(1))\n",
    "sorted_freqlist = sorted(freqlist.items(), key=lambda elem: elem[1],reverse=True)\n",
    "print(sorted_freqlist[0:10]) #freq 내림차순정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률론적 언어 모형\n",
    "확률론적 언어 모형(Probabilistic Language Model)은  mm 개의 단어  w1,w2,…,wmw1,w2,…,wm열(word sequence)이 \n",
    "주어졌을 때 문장으로써 성립될 확률  P(w1,w2,…,wm)P(w1,w2,…,wm)  을 출력함으로써 이 단어 열이 실제로 현실에서 \n",
    "사용될 수 있는 문장(sentence)인지를 판별하는 모형이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__유니그램 모형 (Unigram Model)¶__\n",
    "\n",
    "만약 모든 단어의 활용이 완전히 서로 독립이라면 단어 열의 확률은 다음과 같이 각 단어의 확률의 곱이 된다. 이러한 모형을 유니그램 모형 (Unigram Model)이라고 한다.\n",
    "\n",
    "$P(w_1,w_2,…,w_m)=\\prod_{i=1}^m P(w_i)$\n",
    " \n",
    "__바이그램 모형 (Bigram Model)¶__\n",
    "\n",
    "만약 단어의 활용이 바로 전 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 Bigram 모형 또는 마코프 모형(Markov Model)이라고 한다.\n",
    "\n",
    "$P(w_1,w_2,…,w_m)=P(w_1)\\prod_{i=2}^m P(w_i|w_{i−1})$\n",
    " \n",
    "__N-그램 모형 (N-gram Model)¶__\n",
    "\n",
    "만약 단어의 활용이 바로 전  n 개의 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 N-gram 모형이라고 한다.\n",
    "$P(w_1,w_2,…,w_m)=P(w_1)\\prod_{i=n}^m P(w_i|w_{i−1},…,w_{i−n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률 추정 방법\n",
    "실제 텍스트 코퍼스(corpus)에서 확률을 추정하는 방법은 다음과 같다. 여기에서는 바이그램의 경우를 살펴본다.\n",
    "\n",
    "일단 모든 문장에 문장의 시작과 끝을 나타내는 특별 토큰을 추가한다. \n",
    "예를 들어 문장의 시작은 SS, 문장의 끝은 SE 이라는 토큰을 사용할 수 있다.\n",
    "\n",
    "바이그램 모형에서는 전체 문장의 확률은 다음과 같이 조건부 확률의 곱으로 나타난다.\n",
    "\n",
    "P(SS I am a boy SE)=P(I|SS)⋅P(am|I)⋅P(a|am)⋅P(boy|a)⋅P(SE|boy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제\n",
    "다음은 nltk 패키지의 샘플 코퍼스인 movie_reviews의 텍스트를 기반으로 N-그램 모형을 추정하고 모형 확률로부터 \n",
    "랜덤하게 문장을 생성하는 예제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 문서를 문장으로 분리\n",
    "sentences = list(movie_reviews.sents())\n",
    "\n",
    "import random\n",
    "# 섞는다.\n",
    "random.seed(180607)\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wayne', \"'\", 's', 'greatest', 'ambition', 'is', 'to', 'become', 'a', 'night']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 입력으로부터 확률값을 추정한다.\n",
    "import collections, math\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "\n",
    "def stringify_context(context):\n",
    "    return(\" \".join(context))\n",
    "\n",
    "\n",
    "boundaryToken = \"\"\n",
    "def ngrams(n, sentences, boundaryToken=boundaryToken, verbose=False):\n",
    "    c = {}\n",
    "    q = []\n",
    "    for i in range(n-1):\n",
    "        q.append(boundaryToken)\n",
    "    for sentence in sentences:\n",
    "        for w in sentence + [boundaryToken]:\n",
    "            context_gram = stringify_context(q)\n",
    "            if verbose:\n",
    "                print(q)\n",
    "                print(context_gram)\n",
    "                print(w)\n",
    "            if not context_gram in c:\n",
    "                c[context_gram] = Counter() #Counter 클래스의 객체 c\n",
    "            c[context_gram][w] += 1\n",
    "            q.pop(0)\n",
    "            q.append(w)\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'\": 7,\n",
       "         'are': 4,\n",
       "         'can': 3,\n",
       "         'do': 1,\n",
       "         'don': 1,\n",
       "         'feel': 1,\n",
       "         'focus': 1,\n",
       "         'get': 2,\n",
       "         'have': 2,\n",
       "         'join': 1,\n",
       "         'just': 1,\n",
       "         'know': 2,\n",
       "         'later': 1,\n",
       "         'learn': 1,\n",
       "         'like': 1,\n",
       "         'might': 1,\n",
       "         'never': 1,\n",
       "         'should': 1,\n",
       "         'were': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(2, sentences[:1000])[\"we\"] #Counter 객체를 리턴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BigramModel:\n",
    "    \n",
    "    def __init__(self, training_sentences, smoothing='none'):\n",
    "        train = ngrams(2, training_sentences) #앞서 정의된 ngram함수\n",
    "        self.probs = {}\n",
    "        if smoothing == 'none':\n",
    "            for context_gram in train.keys():\n",
    "                N = sum(train[context_gram].values())\n",
    "                self.probs[context_gram] = Counter({k:v/N for k,v in train[context_gram].items()})\n",
    "\n",
    "    def prob(self, word, context):\n",
    "        \"\"\"takes a word string and a context which is a list of word strings, and returns the probability of the word\"\"\"\n",
    "        c = stringify_context(context)\n",
    "        return(self.probs[c][word])\n",
    "\n",
    "    def scoreSentence(self, sentence, verbose=False):\n",
    "        context = [boundaryToken]\n",
    "        result = 0\n",
    "        for w in sentence + [boundaryToken]:\n",
    "            lp = log(self.prob(w, context))\n",
    "            result = result + lp\n",
    "            if verbose:\n",
    "                pprint([context, w, lp])\n",
    "            context = [w]\n",
    "        return result\n",
    "\n",
    "    def generateSentence(self, verbose=False, goryDetails=False):\n",
    "        context = [boundaryToken]\n",
    "        result = []\n",
    "        w = None\n",
    "        while not w == boundaryToken:\n",
    "            r = random.random() # returns a random float between 0 and 1\n",
    "            x = 0\n",
    "            c = self.probs[stringify_context(context)] # this will be a Counter\n",
    "            w = c.keys()[np.argmax(np.random.multinomial(1, c.values(), (1,))[0])]\n",
    "            result.append(w)\n",
    "            context = [w]\n",
    "            if verbose:\n",
    "                print(w)\n",
    "        result.pop() # drop the boundary token\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramModel(sentences)  #모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018562267971650354"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트레이닝이 끝나면 조건부 확률의 값을 보거나 샘플 문장을 입력해서 문장의 로그 확률을 구할 수 있다.\n",
    "\n",
    "# \"i\" 라는 단어가 나온 뒤에 \"am\"이라는 단어가 나올 확률을 계산하면\n",
    "m.prob(\"am\", [\"i\"]) #조건부확률의 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624749529418908"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob(\"\", [\".\"])  # .(마침표) 뒤에 문장이 끝날 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26083768673815416\n",
      "0.0001437363613793464\n"
     ]
    }
   ],
   "source": [
    "print(m.prob(\"the\", [\"in\"]))  # in 뒤에 the 가 올 확률\n",
    "print(m.prob(\"in\", [\"the\"]))  # the 뒤에 in 이 올 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'\": 0.03571428571428571,\n",
       "         ',': 0.17857142857142858,\n",
       "         '-': 0.07142857142857142,\n",
       "         '.': 0.10714285714285714,\n",
       "         'cider': 0.03571428571428571,\n",
       "         'computer': 0.03571428571428571,\n",
       "         'hasn': 0.03571428571428571,\n",
       "         'in': 0.03571428571428571,\n",
       "         'orchard': 0.03571428571428571,\n",
       "         'picker': 0.03571428571428571,\n",
       "         'pickers': 0.07142857142857142,\n",
       "         'pie': 0.2857142857142857,\n",
       "         'that': 0.03571428571428571})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.probs[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], 'in', -3.7639298908174825]\n",
      "[['in'], 'the', -1.343856955005301]\n",
      "[['the'], '1970s', -9.45366556371934]\n",
      "[['1970s'], '.', -1.413693335308005]\n",
      "[['.'], '', -0.038247236076315826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-16.013392980926444"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = ['in', 'the', '1970s', '.']\n",
    "m.scoreSentence(test_sentence, verbose=True) \n",
    "#각각 로그확률을 계산해 더한 score를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], 'i', -3.3655219750193166]\n",
      "[['i'], 'am', -3.9866243623410944]\n",
      "[['am'], 'a', -2.6441463991227296]\n",
      "[['a'], 'boy', -7.370073198683084]\n",
      "[['boy'], '.', -2.4904468301636156]\n",
      "[['.'], '', -0.038247236076315826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-19.895060001406158"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.scoreSentence([\"i\", \"am\", \"a\", \"boy\", \".\"], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률론적 언어 모형의 활용\n",
    "확률론적 언어 모형은 다음과 같은 분야에 광범위하게 활용할 수 있다.\n",
    "\n",
    "철자 및 문법 교정(Spell Correction)\n",
    "음성 인식(Speech Recognition)\n",
    "자동 번역(Machine Translation)\n",
    "자동 요약(Summarization)\n",
    "챗봇(Question-Answering)\n",
    "\n",
    "https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##w word2vec을 쓰기 위해서 gensim을 다운받아야 한다\n",
    "## terminal에 easy_install -U gensim 혹은 pip install --upgrade gensim을 친다\n",
    "\n",
    "#설치오류시 참고\n",
    "##https://blog.naver.com/sans223/221274010123 -> gensim 설치\n",
    "##https://blog.naver.com/ddonae_/221190968528 -> gensim 설치\n",
    "##https://blog.naver.com/vangarang/220934552201 ->nltk 설치\n",
    "\n",
    "from glob import glob\n",
    "from codecs import open as codecs_open\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from os import path, mkdir\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.manifold import TSNE\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook1.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook2.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook3.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook4.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook5.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook6.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook7.txt',\n",
       " 'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\180607_텍마_Python\\\\books\\\\HPBook8.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 경로를 지정하고 파일을 읽어온다 바탕화면에 다운로드 할 경우 Users와 user를 본인 컴에 맞게 지정할 것\n",
    "\n",
    "def read_books(location):\n",
    "    if path.exists(location):\n",
    "        return sorted(glob(path.join(location, \"*.txt\")))\n",
    "    else:\n",
    "        raise NotADirectoryError(location)\n",
    "books = read_books(r'''C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books''')\n",
    "read_books(r'''C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook1.txt\n",
      "Corpus is now 449988 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook2.txt\n",
      "Corpus is now 949340 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook3.txt\n",
      "Corpus is now 1575163 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook4.txt\n",
      "Corpus is now 2688627 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook5.txt\n",
      "Corpus is now 4204918 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook6.txt\n",
      "Corpus is now 5209621 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook7.txt\n",
      "Corpus is now 6404966 characters long\n",
      "Reading C:\\Users\\Chankoo\\Desktop\\180607_텍마_Python\\books\\HPBook8.txt\n",
      "Corpus is now 6681889 characters long\n"
     ]
    }
   ],
   "source": [
    "## corpus 생성\n",
    "def create_corpus(books):\n",
    "    raw_corpus = u''\n",
    "    for book in books:\n",
    "        print(\"Reading {0}\".format(book))\n",
    "        with codecs_open(book, 'r', 'utf-8') as book_file:\n",
    "            raw_corpus += book_file.read()\n",
    "        print(\"Corpus is now {0} characters long\".format(len(raw_corpus)))\n",
    "    return raw_corpus\n",
    "raw_corpus2 = create_corpus(books)\n",
    "\n",
    "##뒤에 불용어 처리할때 (stop_word) 소문자 감안 하여 미리 소문자로 바꿔준다 (나중에 바꾸는 것을 추천, 대문자 필요할 때 있으니까)\n",
    "raw_corpus = raw_corpus2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장 단위의 token을 생성 \n",
    "def tokenize_corpus(raw_corpus):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    return tokenizer.tokenize(raw_corpus)\n",
    "\n",
    "tokenize_corpus(raw_corpus)\n",
    "token = tokenize_corpus(raw_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'one',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nthe',\n",
       " 'boy',\n",
       " 'lived',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nmr',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'number',\n",
       " 'four',\n",
       " 'privet',\n",
       " 'drive',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'perfectly',\n",
       " 'normal',\n",
       " 'thank',\n",
       " 'much',\n",
       " 'last',\n",
       " 'people',\n",
       " 'expect',\n",
       " 'involved',\n",
       " 'anything',\n",
       " 'strange',\n",
       " 'mysterious',\n",
       " 'hold',\n",
       " 'nonsense',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'director',\n",
       " 'firm',\n",
       " 'called',\n",
       " 'grunnings',\n",
       " 'made',\n",
       " 'drills',\n",
       " 'big',\n",
       " 'beefy',\n",
       " 'man',\n",
       " 'hardly',\n",
       " 'neck',\n",
       " 'although',\n",
       " 'large',\n",
       " 'mustache',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'thin',\n",
       " 'blonde',\n",
       " 'nearly',\n",
       " 'twice',\n",
       " 'usual',\n",
       " 'amount',\n",
       " 'neck',\n",
       " 'came',\n",
       " 'useful',\n",
       " 'spent',\n",
       " 'much',\n",
       " 'time',\n",
       " 'craning',\n",
       " 'garden',\n",
       " 'fences',\n",
       " 'spying',\n",
       " 'neighbors',\n",
       " 'dursleys',\n",
       " 'small',\n",
       " 'son',\n",
       " 'called',\n",
       " 'dudley',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nopinion',\n",
       " 'finer',\n",
       " 'boy',\n",
       " 'anywhere',\n",
       " 'dursleys',\n",
       " 'everything',\n",
       " 'wanted',\n",
       " 'also',\n",
       " 'secret',\n",
       " 'greatest',\n",
       " 'fear',\n",
       " 'somebody',\n",
       " 'would',\n",
       " 'discover',\n",
       " 'think',\n",
       " 'could',\n",
       " 'bear',\n",
       " 'anyone',\n",
       " 'found',\n",
       " 'potters',\n",
       " 'mrs',\n",
       " 'potter',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'sister',\n",
       " 'met',\n",
       " 'several',\n",
       " 'years',\n",
       " 'fact',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'pretended',\n",
       " 'sister',\n",
       " 'sister',\n",
       " 'good',\n",
       " 'nothing',\n",
       " 'husband',\n",
       " 'undursleyish',\n",
       " 'possible',\n",
       " 'dursleys',\n",
       " 'shuddered',\n",
       " 'think',\n",
       " 'neighbors',\n",
       " 'would',\n",
       " 'say',\n",
       " 'potters',\n",
       " 'arrived',\n",
       " 'street',\n",
       " 'dursleys',\n",
       " 'knew',\n",
       " 'potters',\n",
       " 'small',\n",
       " 'son',\n",
       " 'never',\n",
       " 'even',\n",
       " 'seen',\n",
       " 'boy',\n",
       " 'another',\n",
       " 'good',\n",
       " 'reason',\n",
       " 'keeping',\n",
       " 'potters',\n",
       " 'away',\n",
       " 'want',\n",
       " 'dudley',\n",
       " 'mixing',\n",
       " 'child',\n",
       " 'like',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'woke',\n",
       " 'dull',\n",
       " 'gray',\n",
       " 'tuesday',\n",
       " 'story',\n",
       " 'starts',\n",
       " 'nothing',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " 'outside',\n",
       " 'suggest',\n",
       " 'strange',\n",
       " 'mysterious',\n",
       " 'things',\n",
       " 'would',\n",
       " 'soon',\n",
       " 'happening',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'ncountry',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'hummed',\n",
       " 'picked',\n",
       " 'boring',\n",
       " 'tie',\n",
       " 'work',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'gossiped',\n",
       " 'away',\n",
       " 'happily',\n",
       " 'wrestled',\n",
       " 'screaming',\n",
       " 'dudley',\n",
       " 'high',\n",
       " 'chair',\n",
       " 'none',\n",
       " 'noticed',\n",
       " 'large',\n",
       " 'tawny',\n",
       " 'owl',\n",
       " 'flutter',\n",
       " 'past',\n",
       " 'window',\n",
       " 'half',\n",
       " 'past',\n",
       " 'eight',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'picked',\n",
       " 'briefcase',\n",
       " 'pecked',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'cheek',\n",
       " 'tried',\n",
       " 'kiss',\n",
       " 'dudley',\n",
       " 'good',\n",
       " 'bye',\n",
       " 'missed',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nbecause',\n",
       " 'dudley',\n",
       " 'tantrum',\n",
       " 'throwing',\n",
       " 'cereal',\n",
       " 'walls',\n",
       " 'little',\n",
       " 'tyke',\n",
       " 'chortled',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'left',\n",
       " 'house',\n",
       " 'got',\n",
       " 'car',\n",
       " 'backed',\n",
       " 'number',\n",
       " 'four',\n",
       " 'drive',\n",
       " 'corner',\n",
       " 'street',\n",
       " 'noticed',\n",
       " 'first',\n",
       " 'sign',\n",
       " 'something',\n",
       " 'peculiar',\n",
       " 'cat',\n",
       " 'reading',\n",
       " 'map',\n",
       " 'second',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'realize',\n",
       " 'seen',\n",
       " 'jerked',\n",
       " 'head',\n",
       " 'around',\n",
       " 'look',\n",
       " 'tabby',\n",
       " 'cat',\n",
       " 'standing',\n",
       " 'corner',\n",
       " 'privet',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'ndrive',\n",
       " 'map',\n",
       " 'sight',\n",
       " 'could',\n",
       " 'thinking',\n",
       " 'must',\n",
       " 'trick',\n",
       " 'light',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'blinked',\n",
       " 'stared',\n",
       " 'cat',\n",
       " 'stared',\n",
       " 'back',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'drove',\n",
       " 'around',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'ncorner',\n",
       " 'road',\n",
       " 'watched',\n",
       " 'cat',\n",
       " 'mirror',\n",
       " 'reading',\n",
       " 'sign',\n",
       " 'said',\n",
       " 'privet',\n",
       " 'drive',\n",
       " 'looking',\n",
       " 'sign',\n",
       " 'cats',\n",
       " 'read',\n",
       " 'maps',\n",
       " 'signs',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'gave',\n",
       " 'little',\n",
       " 'shake',\n",
       " 'put',\n",
       " 'cat',\n",
       " 'mind',\n",
       " 'drove',\n",
       " 'toward',\n",
       " 'town',\n",
       " 'thought',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nnothing',\n",
       " 'except',\n",
       " 'large',\n",
       " 'order',\n",
       " 'drills',\n",
       " 'hoping',\n",
       " 'get',\n",
       " 'day',\n",
       " 'edge',\n",
       " 'town',\n",
       " 'drills',\n",
       " 'driven',\n",
       " 'mind',\n",
       " 'something',\n",
       " 'else',\n",
       " 'sat',\n",
       " 'usual',\n",
       " 'morning',\n",
       " 'traffic',\n",
       " 'jam',\n",
       " 'help',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nnoticing',\n",
       " 'seemed',\n",
       " 'lot',\n",
       " 'strangely',\n",
       " 'dressed',\n",
       " 'people',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nabout',\n",
       " 'people',\n",
       " 'cloaks',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'bear',\n",
       " 'people',\n",
       " 'dressed',\n",
       " 'funny',\n",
       " 'clothes',\n",
       " 'getups',\n",
       " 'saw',\n",
       " 'young',\n",
       " 'people',\n",
       " 'supposed',\n",
       " 'stupid',\n",
       " 'new',\n",
       " 'fashion',\n",
       " 'drummed',\n",
       " 'fingers',\n",
       " 'steering',\n",
       " 'wheel',\n",
       " 'eyes',\n",
       " 'fell',\n",
       " 'huddle',\n",
       " 'weirdos',\n",
       " 'standing',\n",
       " 'quite',\n",
       " 'close',\n",
       " 'whispering',\n",
       " 'excitedly',\n",
       " 'together',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nenraged',\n",
       " 'see',\n",
       " 'couple',\n",
       " 'young',\n",
       " 'man',\n",
       " 'older',\n",
       " 'wearing',\n",
       " 'emerald',\n",
       " 'green',\n",
       " 'cloak',\n",
       " 'nerve',\n",
       " 'struck',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'probably',\n",
       " 'silly',\n",
       " 'stunt',\n",
       " 'people',\n",
       " 'obviously',\n",
       " 'collecting',\n",
       " 'something',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nyes',\n",
       " 'would',\n",
       " 'traffic',\n",
       " 'moved',\n",
       " 'minutes',\n",
       " 'later',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'arrived',\n",
       " 'grunnings',\n",
       " 'parking',\n",
       " 'lot',\n",
       " 'mind',\n",
       " 'back',\n",
       " 'drills',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'always',\n",
       " 'sat',\n",
       " 'back',\n",
       " 'window',\n",
       " 'office',\n",
       " 'ninth',\n",
       " 'floor',\n",
       " 'might',\n",
       " 'found',\n",
       " 'harder',\n",
       " 'concentrate',\n",
       " 'drills',\n",
       " 'morning',\n",
       " 'see',\n",
       " 'owls',\n",
       " 'swoop',\n",
       " 'ing',\n",
       " 'past',\n",
       " 'broad',\n",
       " 'daylight',\n",
       " 'though',\n",
       " 'people',\n",
       " 'street',\n",
       " 'pointed',\n",
       " 'gazed',\n",
       " 'open',\n",
       " 'mouthed',\n",
       " 'owl',\n",
       " 'owl',\n",
       " 'sped',\n",
       " 'overhead',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'owl',\n",
       " 'even',\n",
       " 'nighttime',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'however',\n",
       " 'perfectly',\n",
       " 'normal',\n",
       " 'owl',\n",
       " 'free',\n",
       " 'morning',\n",
       " 'yelled',\n",
       " 'five',\n",
       " 'different',\n",
       " 'people',\n",
       " 'made',\n",
       " 'several',\n",
       " 'important',\n",
       " 'telephone',\n",
       " 'calls',\n",
       " 'shouted',\n",
       " 'bit',\n",
       " 'good',\n",
       " 'mood',\n",
       " 'lunchtime',\n",
       " 'thought',\n",
       " 'stretch',\n",
       " 'legs',\n",
       " 'walk',\n",
       " 'across',\n",
       " 'road',\n",
       " 'buy',\n",
       " 'bun',\n",
       " 'bakery',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nhe',\n",
       " 'forgotten',\n",
       " 'people',\n",
       " 'cloaks',\n",
       " 'passed',\n",
       " 'group',\n",
       " 'next',\n",
       " 'baker',\n",
       " 'eyed',\n",
       " 'angrily',\n",
       " 'passed',\n",
       " 'know',\n",
       " 'made',\n",
       " 'uneasy',\n",
       " 'bunch',\n",
       " 'whispering',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nexcitedly',\n",
       " 'see',\n",
       " 'single',\n",
       " 'collecting',\n",
       " 'tin',\n",
       " 'way',\n",
       " 'back',\n",
       " 'past',\n",
       " 'clutching',\n",
       " 'large',\n",
       " 'doughnut',\n",
       " 'bag',\n",
       " 'caught',\n",
       " 'words',\n",
       " 'saying',\n",
       " 'potters',\n",
       " 'right',\n",
       " 'heard',\n",
       " 'yes',\n",
       " 'son',\n",
       " 'harry',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nmr',\n",
       " 'dursley',\n",
       " 'stopped',\n",
       " 'dead',\n",
       " 'fear',\n",
       " 'flooded',\n",
       " 'looked',\n",
       " 'back',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nwhisperers',\n",
       " 'wanted',\n",
       " 'say',\n",
       " 'something',\n",
       " 'thought',\n",
       " 'better',\n",
       " 'dashed',\n",
       " 'back',\n",
       " 'across',\n",
       " 'road',\n",
       " 'hurried',\n",
       " 'office',\n",
       " 'snapped',\n",
       " 'secretary',\n",
       " 'disturb',\n",
       " 'seized',\n",
       " 'telephone',\n",
       " 'almost',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nfinished',\n",
       " 'dialing',\n",
       " 'home',\n",
       " 'number',\n",
       " 'changed',\n",
       " 'mind',\n",
       " 'put',\n",
       " 'receiver',\n",
       " 'back',\n",
       " 'stroked',\n",
       " 'mustache',\n",
       " 'thinking',\n",
       " 'stupid',\n",
       " 'potter',\n",
       " 'unusual',\n",
       " 'name',\n",
       " 'sure',\n",
       " 'lots',\n",
       " 'people',\n",
       " 'called',\n",
       " 'potter',\n",
       " 'son',\n",
       " 'called',\n",
       " 'harry',\n",
       " 'come',\n",
       " 'think',\n",
       " 'even',\n",
       " 'sure',\n",
       " 'nephew',\n",
       " 'called',\n",
       " 'harry',\n",
       " 'never',\n",
       " 'even',\n",
       " 'seen',\n",
       " 'boy',\n",
       " 'might',\n",
       " 'harvey',\n",
       " 'harold',\n",
       " 'point',\n",
       " 'worrying',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'always',\n",
       " 'got',\n",
       " 'upset',\n",
       " 'mention',\n",
       " 'sister',\n",
       " 'blame',\n",
       " 'sister',\n",
       " 'like',\n",
       " 'people',\n",
       " 'cloaks',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nhe',\n",
       " 'found',\n",
       " 'lot',\n",
       " 'harder',\n",
       " 'concentrate',\n",
       " 'drills',\n",
       " 'afternoon',\n",
       " 'left',\n",
       " 'building',\n",
       " 'five',\n",
       " 'clock',\n",
       " 'still',\n",
       " 'worried',\n",
       " 'walked',\n",
       " 'straight',\n",
       " 'someone',\n",
       " 'outside',\n",
       " 'door',\n",
       " 'sorry',\n",
       " 'grunted',\n",
       " 'tiny',\n",
       " 'old',\n",
       " 'man',\n",
       " 'stumbled',\n",
       " 'almost',\n",
       " 'fell',\n",
       " 'seconds',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'realized',\n",
       " 'man',\n",
       " 'wearing',\n",
       " 'violet',\n",
       " 'cloak',\n",
       " 'seem',\n",
       " 'upset',\n",
       " 'almost',\n",
       " 'knocked',\n",
       " 'ground',\n",
       " 'contrary',\n",
       " 'face',\n",
       " 'split',\n",
       " 'wide',\n",
       " 'smile',\n",
       " 'said',\n",
       " 'squeaky',\n",
       " 'voice',\n",
       " 'made',\n",
       " 'passersby',\n",
       " 'stare',\n",
       " 'sorry',\n",
       " 'dear',\n",
       " 'sir',\n",
       " 'nothing',\n",
       " 'could',\n",
       " 'upset',\n",
       " 'today',\n",
       " 'rejoice',\n",
       " 'know',\n",
       " 'gone',\n",
       " 'last',\n",
       " 'even',\n",
       " 'muggles',\n",
       " 'like',\n",
       " 'celebrating',\n",
       " 'happy',\n",
       " 'happy',\n",
       " 'day',\n",
       " 'old',\n",
       " 'man',\n",
       " 'hugged',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'around',\n",
       " 'middle',\n",
       " 'walked',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'stood',\n",
       " 'rooted',\n",
       " 'spot',\n",
       " 'hugged',\n",
       " 'complete',\n",
       " 'stranger',\n",
       " 'also',\n",
       " 'thought',\n",
       " 'called',\n",
       " 'muggle',\n",
       " 'whatever',\n",
       " 'rattled',\n",
       " 'hurried',\n",
       " 'car',\n",
       " 'set',\n",
       " 'home',\n",
       " 'hoping',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nhe',\n",
       " 'imagining',\n",
       " 'things',\n",
       " 'never',\n",
       " 'hoped',\n",
       " 'approve',\n",
       " 'imagination',\n",
       " 'pulled',\n",
       " 'driveway',\n",
       " 'number',\n",
       " 'four',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'saw',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nand',\n",
       " 'improve',\n",
       " 'mood',\n",
       " 'tabby',\n",
       " 'cat',\n",
       " 'spotted',\n",
       " 'morning',\n",
       " 'sitting',\n",
       " 'garden',\n",
       " 'wall',\n",
       " 'sure',\n",
       " 'one',\n",
       " 'markings',\n",
       " 'around',\n",
       " 'eyes',\n",
       " 'shoo',\n",
       " 'said',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'loudly',\n",
       " 'cat',\n",
       " 'move',\n",
       " 'gave',\n",
       " 'stern',\n",
       " 'look',\n",
       " 'normal',\n",
       " 'cat',\n",
       " 'behavior',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'wondered',\n",
       " 'trying',\n",
       " 'pull',\n",
       " 'together',\n",
       " 'let',\n",
       " 'house',\n",
       " 'still',\n",
       " 'determined',\n",
       " 'mention',\n",
       " 'anything',\n",
       " 'wife',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'nice',\n",
       " 'normal',\n",
       " 'day',\n",
       " 'told',\n",
       " 'dinner',\n",
       " 'mrs',\n",
       " 'next',\n",
       " 'door',\n",
       " 'problems',\n",
       " 'daughter',\n",
       " 'dudley',\n",
       " 'learned',\n",
       " 'new',\n",
       " 'word',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'tried',\n",
       " 'act',\n",
       " 'normally',\n",
       " 'dudley',\n",
       " 'put',\n",
       " 'bed',\n",
       " 'went',\n",
       " 'living',\n",
       " 'room',\n",
       " 'time',\n",
       " 'catch',\n",
       " 'last',\n",
       " 'report',\n",
       " 'evening',\n",
       " 'news',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'n',\n",
       " 'finally',\n",
       " 'bird',\n",
       " 'watchers',\n",
       " 'everywhere',\n",
       " 'reported',\n",
       " 'nation',\n",
       " 'owls',\n",
       " 'behaving',\n",
       " 'unusually',\n",
       " 'today',\n",
       " 'although',\n",
       " 'owls',\n",
       " 'normally',\n",
       " 'hunt',\n",
       " 'night',\n",
       " 'hardly',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'daylight',\n",
       " 'hundreds',\n",
       " 'sightings',\n",
       " 'birds',\n",
       " 'flying',\n",
       " 'every',\n",
       " 'direction',\n",
       " 'since',\n",
       " 'sunrise',\n",
       " 'experts',\n",
       " 'unable',\n",
       " 'explain',\n",
       " 'owls',\n",
       " 'suddenly',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nchanged',\n",
       " 'sleeping',\n",
       " 'pattern',\n",
       " 'newscaster',\n",
       " 'allowed',\n",
       " 'grin',\n",
       " 'mysterious',\n",
       " 'jim',\n",
       " 'mcguffin',\n",
       " 'weather',\n",
       " 'going',\n",
       " 'showers',\n",
       " 'owls',\n",
       " 'tonight',\n",
       " 'jim',\n",
       " 'well',\n",
       " 'ted',\n",
       " 'said',\n",
       " 'weatherman',\n",
       " 'know',\n",
       " 'owls',\n",
       " 'acting',\n",
       " 'oddly',\n",
       " 'today',\n",
       " 'viewers',\n",
       " 'far',\n",
       " 'apart',\n",
       " 'kent',\n",
       " 'yorkshire',\n",
       " 'dundee',\n",
       " 'phoning',\n",
       " 'tell',\n",
       " 'instead',\n",
       " 'rain',\n",
       " 'promised',\n",
       " 'yesterday',\n",
       " 'downpour',\n",
       " 'shooting',\n",
       " 'stars',\n",
       " 'perhaps',\n",
       " 'people',\n",
       " 'celebrating',\n",
       " 'bonfire',\n",
       " 'night',\n",
       " 'early',\n",
       " 'next',\n",
       " 'week',\n",
       " 'folks',\n",
       " 'promise',\n",
       " 'wet',\n",
       " 'night',\n",
       " 'tonight',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'sat',\n",
       " 'frozen',\n",
       " 'armchair',\n",
       " 'shooting',\n",
       " 'stars',\n",
       " 'britain',\n",
       " 'owls',\n",
       " 'flying',\n",
       " 'daylight',\n",
       " 'mysterious',\n",
       " 'people',\n",
       " 'cloaks',\n",
       " 'place',\n",
       " 'whisper',\n",
       " 'whisper',\n",
       " 'potters',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nmrs',\n",
       " 'dursley',\n",
       " 'came',\n",
       " 'living',\n",
       " 'room',\n",
       " 'carrying',\n",
       " 'two',\n",
       " 'cups',\n",
       " 'tea',\n",
       " 'good',\n",
       " 'say',\n",
       " 'something',\n",
       " 'cleared',\n",
       " 'throat',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nnervously',\n",
       " 'er',\n",
       " 'petunia',\n",
       " 'dear',\n",
       " 'heard',\n",
       " 'sister',\n",
       " 'lately',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'nas',\n",
       " 'expected',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'looked',\n",
       " 'shocked',\n",
       " 'angry',\n",
       " 'normally',\n",
       " 'pretended',\n",
       " 'sister',\n",
       " 'said',\n",
       " 'sharply',\n",
       " 'funny',\n",
       " 'stuff',\n",
       " 'news',\n",
       " 'mr',\n",
       " 'dursley',\n",
       " 'mumbled',\n",
       " 'owls',\n",
       " 'shooting',\n",
       " 'stars',\n",
       " 'lot',\n",
       " 'funny',\n",
       " 'looking',\n",
       " 'people',\n",
       " 'town',\n",
       " 'today',\n",
       " 'r',\n",
       " 'n',\n",
       " 'r',\n",
       " 'n',\n",
       " 'snapped',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'well',\n",
       " 'thought',\n",
       " 'maybe',\n",
       " 'something',\n",
       " 'know',\n",
       " 'crowd',\n",
       " 'mrs',\n",
       " 'dursley',\n",
       " 'sipped',\n",
       " 'tea',\n",
       " 'pursed',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 단위의 token을 word로 쪼갠다\n",
    "def sentence_to_words_list(token):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", str(token)).split()\n",
    "    return [word for word in words if word not in stop_words]\n",
    "tokens = sentence_to_words_list(token)\n",
    "sentence_to_words_list(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 세글자 이상의 단어만 수집하기로 한다(r, n 이런 것 때문에!) 10분정도 걸린다 \n",
    "def tokens_to_words(tokens):\n",
    "    words = [sentence_to_words_list(token) for token in tokens if len(token) > 2]\n",
    "    print(\"The corpus contains {0:,} tokens\".format(sum([len(word) for word in words])))\n",
    "    return words\n",
    "words = tokens_to_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## workers = multiprocessing.cpu_count(), min_count = 50 등장횟수 50이하인 단어는 제외, size=100 100개의 차원으로 embedding\n",
    "## sg=0 이면 CBOW sg=1이면 skip.gram  (sg=skip.gram)\n",
    "def build_vocab(words, num_features, min_word_count, num_workers, context_size):\n",
    "    word2vec = Word2Vec(sg=1, workers=num_workers, size=num_features, min_count=min_word_count,\n",
    "                        window=context_size)\n",
    "    print(\"Building Vocabulary\")\n",
    "    word2vec.build_vocab(words)\n",
    "    return word2vec\n",
    "\n",
    "model = build_vocab(words,100,50,4,10)\n",
    "\n",
    "\n",
    "##모델을 저장하고 불러와서 다시 training시킬 수 있다\n",
    "model.save(r'''C:\\Users\\100\\Desktop\\텍스트마이닝\\6_word2vec\\\\model.w2v''')\n",
    "model = Word2Vec.load(r'''C:\\Users\\100\\Desktop\\텍스트마이닝\\6_word2vec\\model.w2v''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['voldemort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word_vector와 내장함수로 여러가지를 구해볼 수 있다\n",
    "def word_correlation(word_vector, a, b, c):\n",
    "    return word_vector.most_similar_cosmul(positive=[a, c], negative=[b])[0][0]\n",
    "\n",
    "\n",
    "def word_find_most_similar(word_vector, word):\n",
    "    return word_vector.most_similar(word)[0][0]\n",
    "\n",
    "\n",
    "def word_odd_one(word_vector, phrase):\n",
    "    return word_vector.doesnt_match(phrase.split())\n",
    "\n",
    "def similarity(word_vector,a,b):\n",
    "    return word_vector.similarity(a,b)\n",
    "\n",
    "print(word_correlation(model.wv, 'harry', 'voldemort', 'ron'))\n",
    "print(word_find_most_similar(model.wv, 'ron'))\n",
    "print(word_odd_one(model.wv,'He had been hugged by a complete stranger'))\n",
    "print(similarity(model.wv,'harry','ron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시각화하기 mincount 조절해서 selective하게 그리면 된다. 이것도 5분정도 걸린다\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wine2vec \n",
    "https://www.kaggle.com/zynicide/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import multiprocessing\n",
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('winemag-data_first150k.csv')\n",
    "#contains 10 columns and 150k rows of wine reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country                                        description  \\\n",
       "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
       "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
       "4           4  France  This is the top wine from La Bégude, named aft...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "2         Special Selected Late Harvest      96   90.0      California   \n",
       "3                               Reserve      96   65.0          Oregon   \n",
       "4                            La Brûlade      95   66.0        Provence   \n",
       "\n",
       "            region_1           region_2             variety  \\\n",
       "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
       "1               Toro                NaN       Tinta de Toro   \n",
       "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
       "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
       "4             Bandol                NaN  Provence red blend   \n",
       "\n",
       "                    winery  \n",
       "0                    Heitz  \n",
       "1  Bodega Carmen Rodríguez  \n",
       "2                 Macauley  \n",
       "3                    Ponzi  \n",
       "4     Domaine de la Bégude  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['variety']\n",
    "descriptions = data['description']\n",
    "# 맛(oaky, tannic, acidic, berry, etc.)에 대한 description으로 wine의 type(Pinot Noir, Cabernet Sav., etc.) 예측하는 것이 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabernet Sauvignon   :   This tremendous 100% varietal wine hails from Oakville and was aged over three years in oak. Juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. Balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. Enjoy 2022–2030.\n",
      "Sauvignon Blanc   :   Delicious while also young and textured, this wine comes from biodynamically grown grapes. It has a strong sense of minerality as well as intense citrus and green fruits. It's tight at the moment and needs to round out, so drink from 2018.\n",
      "Chardonnay   :   A smoky scent and earthy, crisp-apple flavors make this medium-bodied wine a change of pace from the average butterball Chardonnay. It has welcome acidity and a nicely smooth texture.\n"
     ]
    }
   ],
   "source": [
    "print('{}   :   {}'.format(labels.tolist()[0], descriptions.tolist()[0]))\n",
    "print('{}   :   {}'.format(labels.tolist()[56], descriptions.tolist()[56]))\n",
    "print('{}   :   {}'.format(labels.tolist()[93], descriptions.tolist()[93]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chardonnay                  14482\n",
      "Pinot Noir                  14291\n",
      "Cabernet Sauvignon          12800\n",
      "Red Blend                   10062\n",
      "Bordeaux-style Red Blend     7347\n",
      "Name: variety, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "varietal_counts = labels.value_counts()\n",
    "print(varietal_counts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = \"\"\n",
    "for description in descriptions[:10000]:\n",
    "    corpus_raw += description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw) #[^a-zA-Z] means any character that IS NOT a-z OR A-Z , [^a-zA-Z]를 공백으로 대체\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tart cherry lingers on the finish.A deeper salmon color with elegantly lacy bubbles and a slight cloudy appearance, this sparkler by Norm Yost offers dessicated watermelon, dried orange blossoms, yeast, citrus rinds and fresher strawberry notes on the nose.\n",
      "['Tart', 'cherry', 'lingers', 'on', 'the', 'finish', 'A', 'deeper', 'salmon', 'color', 'with', 'elegantly', 'lacy', 'bubbles', 'and', 'a', 'slight', 'cloudy', 'appearance', 'this', 'sparkler', 'by', 'Norm', 'Yost', 'offers', 'dessicated', 'watermelon', 'dried', 'orange', 'blossoms', 'yeast', 'citrus', 'rinds', 'and', 'fresher', 'strawberry', 'notes', 'on', 'the', 'nose']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[234])\n",
    "print(sentence_to_wordlist(raw_sentences[234]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wine corpus contains 408,741 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print('The wine corpus contains {0:,} tokens'.format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 10\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "context_size = 10\n",
    "downsampling = 1e-3\n",
    "seed=1993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 2612\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec vocabulary length:', len(wine2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17323\n"
     ]
    }
   ],
   "source": [
    "print(wine2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354033, 2043705)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine2vec.train(sentences, total_examples=wine2vec.corpus_count, epochs=wine2vec.epochs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Playing with the Model__\n",
    "\n",
    "Now that we have a trained model we can get to the fun part and start playing around with the results. As you can tell from the outputs below, there is definitely still some noise in the data that could be worked out by tuning the parameters further, but overall we are getting pretty good results.\n",
    "\n",
    "Words closest to a given word\n",
    "\"melon,\" \"berry,\" and \"oak\" are words that someone might use to describe the taste/smell of a wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('papaya', 0.7407190203666687),\n",
       " ('honeydew', 0.7113039493560791),\n",
       " ('banana', 0.6920315027236938),\n",
       " ('cantaloupe', 0.6835112571716309),\n",
       " ('Melon', 0.681281328201294),\n",
       " ('pit', 0.6758646965026855),\n",
       " ('mango', 0.6728077530860901),\n",
       " ('kiwi', 0.6680706739425659),\n",
       " ('bath', 0.6440451145172119),\n",
       " ('mealy', 0.6407906413078308)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine2vec.wv.most_similar('melon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tartness', 0.8361387252807617),\n",
       " ('cloying', 0.8280996084213257),\n",
       " ('watery', 0.8243080377578735),\n",
       " ('tad', 0.8226309418678284),\n",
       " ('punchy', 0.8133155703544617),\n",
       " ('snap', 0.8117392063140869),\n",
       " ('sticky', 0.8109685182571411),\n",
       " ('lacking', 0.8065575361251831),\n",
       " ('flat', 0.803278386592865),\n",
       " ('angular', 0.7976535558700562)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine2vec.wv.most_similar('acidic') #1. 매우 신   2. 산성의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gris', 0.8386585116386414),\n",
       " ('Chenin', 0.8004210591316223),\n",
       " ('Blanc', 0.7875776886940002),\n",
       " ('Grigio', 0.7813867926597595),\n",
       " ('Marsanne', 0.7705585360527039),\n",
       " ('Roussanne', 0.7692981958389282),\n",
       " ('Viognier', 0.7582399249076843),\n",
       " ('Muscat', 0.753264307975769),\n",
       " ('Champagne', 0.7360697984695435),\n",
       " ('Verdejo', 0.7329103946685791)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine2vec.wv.most_similar('Chardonnay') #백포도주의 일종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
